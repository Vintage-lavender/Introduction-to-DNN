{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020314482_인공지능융합전공_황선우_homework#11_1,3번코드.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. ‘봄’ , ‘여름’, ‘가을’, ‘겨울’에 대한 representation 값을 각각 (1,2), (-1,3), (2,-2), (2,1)이라고 했을 때, 아래의 순서에 따른 positional encoding 값을 더한 각 단어의 representation 값을 구하시오. Transformer의 positional encoding을 사용하시오.\n",
        "a. [ ‘여름’, ‘가을’, ‘겨울’, ‘봄’]"
      ],
      "metadata": {
        "id": "PlXJrNH3WHuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 4\n",
        "dim = 2\n",
        "\n",
        "def get_angles(pos, i, dim):\n",
        "  angles = 1/math.pow(10000,(2*(i//2))/dim)\n",
        "  return pos * angles\n",
        "\n",
        "def get_positional_encoding(pos,i,dim):\n",
        "  if i%2==0:\n",
        "    return math.sin(get_angles(pos,i,dim))\n",
        "  return math.cos(get_angles(pos,i,dim))\n",
        "\n",
        "result = [[0] * dim for _ in range(n)]\n",
        "\n",
        "for i in range(n):\n",
        "  for j in range(dim):\n",
        "    result[i][j] = get_positional_encoding(i,j,dim)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "hYntBeFbMknG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "representation = np.array([[-1,3],[2,-2],[2,1],[1,2]])\n",
        "positional_embedding = np.array(result)\n",
        "\n",
        "representation + positional_embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVgwAZjJPgqM",
        "outputId": "747e9aac-26e7-45bb-c5a2-7f9d03d9f1f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.        ,  4.        ],\n",
              "       [ 2.84147098, -1.45969769],\n",
              "       [ 2.90929743,  0.58385316],\n",
              "       [ 1.14112001,  1.0100075 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Transformer 모델에서 vector size = 3 이다.\n",
        "[I , am , a , student] 를 Q, K , V 로 한 결과가 아래와 같을 때 QI에 대한 attention 출력값을 구하시오.\n",
        "- QI = [2, 4, 6]\n",
        "- KI = [2, 4, 6], Kam=[1, 2, 4], Ka=[0, 1, 2], Kstudent=[1, 1, 1]\n",
        "- VI = [0.5, 0.3, 0.2], Vam=[0.6, 0.1, 0.3], Va=[0.1, 0.7, 0.2], Vstudent=[0.4, 0.5, 0.1]"
      ],
      "metadata": {
        "id": "-cfzuF6CWMz1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAE8H2WlI3ZP",
        "outputId": "6b113b52-c07e-41a5-c484-24f44bc52136"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([56., 34., 16., 12.])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "import math\n",
        "\n",
        "Q = torch.FloatTensor([2,4,6])\n",
        "Kt = torch.FloatTensor([[2,1,0,1],[4,2,1,1],[6,4,2,1]])\n",
        "V = torch.FloatTensor([[0.5,0.3,0.2],[0.6,0.1,0.3],[0.1,0.7,0.2],[0.4,0.5,0.1]])\n",
        "embedding_d = 3\n",
        "QKt = torch.matmul(Q,Kt)\n",
        "ad_QKt = QKt/math.sqrt(embedding_d)\n",
        "\n",
        "QKt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ad_QKt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X57EYESSOwLY",
        "outputId": "59e14464-1ce9-44b0-8a3d-e7753c7189b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([32.3316, 19.6299,  9.2376,  6.9282])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "expQKt = torch.exp(ad_QKt)\n",
        "sum_expQKt = torch.sum(expQKt)\n",
        "softmax = expQKt/sum_expQKt\n",
        "\n",
        "softmax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jrz3Hr4wRw2Q",
        "outputId": "4ac0eff0-d1a7-462f-a805-b45af6de1273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000e+00, 3.0459e-06, 9.3411e-11, 9.2776e-12])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.matmul(softmax,V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtqManf-PH7o",
        "outputId": "9878c325-1c93-4364-8ff5-50bfba857c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5000, 0.3000, 0.2000])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}